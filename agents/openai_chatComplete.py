import logging
import openai
from agents.config.openai import API_KEY, BASE_URL, temperature
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential, stop_after_delay,
)
from models.model_config import MODEL_CONFIG


def print_chat_message(messages):
    for message in messages:
        logging.info(f"{message['role']}: {message['content']}")

@retry(wait=wait_random_exponential(min=0.02, max=1), stop=(stop_after_delay(10) | stop_after_attempt(100)))
def completion_with_backoff(messages, model_type):

    if model_type in MODEL_CONFIG.keys():

        port = MODEL_CONFIG[model_type]['port']
        model_full_path= MODEL_CONFIG[model_type]['model']
        
        openai_api_key = "EMPTY"
        openai_api_base = f"http://localhost:{port}/v1"

        client = openai.OpenAI(

            api_key=openai_api_key,
            base_url=openai_api_base,
        )
        try:
            response = client.chat.completions.create(

                model=model_full_path,
                messages=messages,
                temperature=temperature,
                timeout=30*60,
                max_tokens=4096,
                
            )
            result = response.choices[0].message
            answer = result.content
            return answer
        except KeyError:

            return None
        except openai.BadRequestError as e:

            return e

            

    else:
        openai.api_key = API_KEY
        openai.base_url = BASE_URL

        try:
            response = openai.chat.completions.create(
            model=model_type,
            messages=messages,
            temperature=temperature,
        )
            result = response.choices[0].message
            answer = result.content
            return answer
        except KeyError:

            return None
        except openai.BadRequestError as e:
            return e

async def completion_with_backoff_async(messages, model_type, temperature):
    if model_type in MODEL_CONFIG.keys():
        port = MODEL_CONFIG[model_type]['port']
        model_full_path = MODEL_CONFIG[model_type]['model']

        openai_api_key = "EMPTY"  # Replace EMPTY with actual API Key if needed
        openai_api_base = f"http://localhost:{port}/v1"

        client = openai.AsyncClient(api_key=openai_api_key, base_url=openai_api_base)
        response = await client.Chat.create(
            model=model_full_path,
            messages=messages,
            temperature=temperature,
            max_tokens=4096,
        )
        answer = response.choices[0].message
        return answer

    else:
        openai.api_key = API_KEY
        openai.base_url = BASE_URL

        response = await openai.Chat.create(
            model=model_type,
            messages=messages,
            temperature=temperature,
        )
        answer = response.choices[0].message
        return answer


def completion_with_log(messages, model_type, enable_log=False):
    if enable_log:
        logging.info('========CHAT HISTORY========')
        print_chat_message(messages)
    response = completion_with_backoff(messages, model_type)
    if enable_log:
        logging.info('========RESPONSE========')
        logging.info(response)
        logging.info('========RESPONSE END========')
    return response

async def completion_with_log_async(messages, model_type, enable_log=False):
    if enable_log:
        logging.info('========CHAT HISTORY========')
        print_chat_message(messages)
    response = await completion_with_backoff_async(messages, model_type)
    if enable_log:
        logging.info('========RESPONSE========')
        logging.info(response)
        logging.info('========RESPONSE END========')
    return response

def completion_for_4v(messages, model_type):
    if model_type in MODEL_CONFIG.keys():
        ## 目前只实现llava
        assert "llava" in model_type
        port = MODEL_CONFIG[model_type]['port']
        model_full_path= MODEL_CONFIG[model_type]['model']        
        openai_api_key = "EMPTY"
        openai_api_base = f"http://localhost:{port}/v1"
        client = openai.OpenAI(
            api_key=openai_api_key,
            base_url=openai_api_base,
        )
        try:
            response = client.chat.completions.create(
                model=model_full_path,
                messages=messages,
                temperature=temperature,
                max_tokens=1000 
            )
            print(response)
            result = response.choices[0].message
            answer = result.content
            return answer
        except KeyError:
            return None
        except openai.BadRequestError as e:
            return e          
    else:
        openai.api_key = API_KEY
        openai.base_url = BASE_URL
        try:
            response = openai.chat.completions.create(
            model=model_type,
            messages=messages,
            temperature=temperature,
            max_tokens=1000
        )
            result = response.choices[0].message
            answer = result.content
            return answer
        except KeyError:
            return None
        except openai.BadRequestError as e:
            return e

async def completion_for_4v_async(messages, model_type):
    if model_type in MODEL_CONFIG.keys():
        ## 目前只实现llava
        assert "llava" in model_type
        port = MODEL_CONFIG[model_type]['port']
        model_full_path= MODEL_CONFIG[model_type]['model']        
        openai_api_key = "EMPTY"
        openai_api_base = f"http://localhost:{port}/v1"
        client = openai.AsyncClient(
            api_key=openai_api_key,
            base_url=openai_api_base,
        )
        try:
            response = await client.chat.completions.create(
                model=model_full_path,
                messages=messages,
                temperature=temperature,
                max_tokens=1000
            )
            result = response.choices[0].message
            answer = result.content
            return answer
        except KeyError:
            return None
        except openai.BadRequestError as e:
            return e          
    else:
        openai.api_key = API_KEY
        openai.base_url = BASE_URL
        try:
            response = openai.chat.completions.create(
            model=model_type,
            messages=messages,
            temperature=temperature,
            max_tokens=1000
        )
            result = response.choices[0].message
            answer = result.content
            return answer
        except KeyError:
            return None
        except openai.BadRequestError as e:
            return e

